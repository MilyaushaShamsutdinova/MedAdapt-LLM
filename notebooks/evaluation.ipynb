{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation\n",
    "\n",
    "This is notebook for evaluation our 3 models we adapted to medical domain using different methods. We choose to evaluate on the set of benchmarks from [Open Medical-LLM Leaderboard](https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard) including:\n",
    "\n",
    "* [MedMCQA](https://huggingface.co/datasets/openlifescienceai/medmcqa) - mcq, 1k from test split\n",
    "* [MedQA](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options-hf) - mcq, 1k from test splits\n",
    "* [MMLU](https://huggingface.co/datasets/cais/mmlu) - mcq, 600 from 6 medical subsets test split\n",
    "* [PubMedQA](https://huggingface.co/datasets/qiaojin/PubMedQA) - qa, 1k from pqa_labeled subset train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:45:50.178440Z",
     "iopub.status.busy": "2025-04-11T15:45:50.177891Z",
     "iopub.status.idle": "2025-04-11T15:49:04.676818Z",
     "shell.execute_reply": "2025-04-11T15:49:04.675753Z",
     "shell.execute_reply.started": "2025-04-11T15:45:50.178414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:04.678711Z",
     "iopub.status.busy": "2025-04-11T15:49:04.678367Z",
     "iopub.status.idle": "2025-04-11T15:49:04.682770Z",
     "shell.execute_reply": "2025-04-11T15:49:04.682204Z",
     "shell.execute_reply.started": "2025-04-11T15:49:04.678690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED=4242"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks loading and preparing for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedMCQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:04.683565Z",
     "iopub.status.busy": "2025-04-11T15:49:04.683400Z",
     "iopub.status.idle": "2025-04-11T15:49:14.011421Z",
     "shell.execute_reply": "2025-04-11T15:49:14.010867Z",
     "shell.execute_reply.started": "2025-04-11T15:49:04.683552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1675a651564ac6911ccc9563f3ec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d0c7c0d22a4c71a85295104f4f3078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/85.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d8ddcfcf0446c0a222d3518d9ce680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/936k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20201309c724363aefd8cc8c5e7e9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2568ed5881b241bd9c51ba10121fcafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/182822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5b5c457d2d4ccc9a800a1870d67b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee2721443a340a88e316299815bf9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_medmcqa = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")\n",
    "ds_medmcqa = ds_medmcqa.shuffle(seed=SEED).select(range(100))  # 1000\n",
    "ds_medmcqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:14.013138Z",
     "iopub.status.busy": "2025-04-11T15:49:14.012742Z",
     "iopub.status.idle": "2025-04-11T15:49:14.019308Z",
     "shell.execute_reply": "2025-04-11T15:49:14.018469Z",
     "shell.execute_reply.started": "2025-04-11T15:49:14.013119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4653fb7a-ddbf-493b-b4ef-92205582a27a',\n",
       " 'question': 'Which of the following tooth is not having 5 cusps?',\n",
       " 'opa': 'Mandibular 2nd Molar',\n",
       " 'opb': 'Mandibular 1st Molar',\n",
       " 'opc': 'Mandibular 3rd Molar',\n",
       " 'opd': 'Maxillary 1st Molar',\n",
       " 'cop': 0,\n",
       " 'choice_type': 'single',\n",
       " 'exp': None,\n",
       " 'subject_name': 'Dental',\n",
       " 'topic_name': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_medmcqa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:14.020393Z",
     "iopub.status.busy": "2025-04-11T15:49:14.020123Z",
     "iopub.status.idle": "2025-04-11T15:49:16.536688Z",
     "shell.execute_reply": "2025-04-11T15:49:16.535932Z",
     "shell.execute_reply.started": "2025-04-11T15:49:14.020369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Formats a single example into a prompt for the LLM.\"\"\"\n",
    "    question = example['question']\n",
    "    options = {\n",
    "        \"A\": example['opa'],\n",
    "        \"B\": example['opb'],\n",
    "        \"C\": example['opc'],\n",
    "        \"D\": example['opd']\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You are an expert in solving multiple-choice questions accurately and explaining your reasoning clearly.\n",
    "        Given a question and a list of answer choices (A, B, C, D), your task is to:\n",
    "        1. Reason shortly about the question and answer choices to find evidances to support your answer.\n",
    "        2. Identify the correct answer. Please choose the single best answer from the options provided.\n",
    "        3. Output the final answer in the format: Answer: [Option Letter]\n",
    "\n",
    "        Question: {question}\n",
    "        Options:\n",
    "        A. {options['A']}\n",
    "        B. {options['B']}\n",
    "        C. {options['C']}\n",
    "        D. {options['D']}\n",
    "\n",
    "        Reasoning:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:16.537744Z",
     "iopub.status.busy": "2025-04-11T15:49:16.537522Z",
     "iopub.status.idle": "2025-04-11T15:49:16.548247Z",
     "shell.execute_reply": "2025-04-11T15:49:16.547630Z",
     "shell.execute_reply.started": "2025-04-11T15:49:16.537727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_ground_truth(example):\n",
    "    \"\"\"Maps the correct option index (cop) to the corresponding letter.\"\"\"\n",
    "    mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "    cop_index = example.get('cop')\n",
    "    if cop_index is None or cop_index not in mapping:\n",
    "        print(f\"Warning: Invalid 'cop' value found: {cop_index} in example ID {example.get('id')}. Skipping ground truth.\")\n",
    "        return None\n",
    "    return mapping[cop_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:16.549165Z",
     "iopub.status.busy": "2025-04-11T15:49:16.548915Z",
     "iopub.status.idle": "2025-04-11T15:49:16.556214Z",
     "shell.execute_reply": "2025-04-11T15:49:16.555600Z",
     "shell.execute_reply.started": "2025-04-11T15:49:16.549143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_choice(generated_text):\n",
    "    \"\"\"Extracts the predicted choice (A, B, C, or D) from the LLM's output.\"\"\"\n",
    "    text = generated_text.strip()\n",
    "\n",
    "    # 1. Check for direct answer at the beginning (e.g., \"A\", \"A.\", \"A)\")\n",
    "    match = re.match(r\"^\\s*([A-D])(?:[.)\\s]|$)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # 2. Check for phrases like \"The answer is A\" or \"Answer: A\"\n",
    "    match = re.search(r'(?:answer|choice|option) is\\s*:?\\s*([A-D])', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # 3. Look for the first standalone letter A, B, C, or D in the text\n",
    "    match = re.search(r'\\b([A-D])\\b', text)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # Fallback: If no clear choice found, return None\n",
    "    print(f\"Warning: Could not extract choice from text: '{text[:1000]}...'\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:16.557397Z",
     "iopub.status.busy": "2025-04-11T15:49:16.557189Z",
     "iopub.status.idle": "2025-04-11T15:49:16.603014Z",
     "shell.execute_reply": "2025-04-11T15:49:16.602433Z",
     "shell.execute_reply.started": "2025-04-11T15:49:16.557374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Prompts and Ground Truths ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting prompts: 100%|██████████| 100/100 [00:00<00:00, 6413.51it/s]\n",
      "Extracting ground truths: 100%|██████████| 100/100 [00:00<00:00, 8365.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 100 prompts for evaluation.\n",
      "\n",
      "Example Prompt:\n",
      "\n",
      "        You are an expert in solving multiple-choice questions accurately and explaining your reasoning clearly.\n",
      "        Given a question and a list of answer choices (A, B, C, D), your task is to:\n",
      "        1. Reason shortly about the question and answer choices to find evidances to support your answer.\n",
      "        2. Identify the correct answer. Please choose the single best answer from the options provided.\n",
      "        3. Output the final answer in the format: Answer: [Option Letter]\n",
      "\n",
      "        Question: Which of the following tooth is not having 5 cusps?\n",
      "        Options:\n",
      "        A. Mandibular 2nd Molar\n",
      "        B. Mandibular 1st Molar\n",
      "        C. Mandibular 3rd Molar\n",
      "        D. Maxillary 1st Molar\n",
      "\n",
      "        Reasoning:\n",
      "    \n",
      "Corresponding Ground Truth: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing Prompts and Ground Truths ---\")\n",
    "prompts = [format_prompt(ex) for ex in tqdm(ds_medmcqa, desc=\"Formatting prompts\")]\n",
    "ground_truths = [get_ground_truth(ex) for ex in tqdm(ds_medmcqa, desc=\"Extracting ground truths\")]\n",
    "valid_indices = [i for i, gt in enumerate(ground_truths) if gt is not None]\n",
    "\n",
    "if len(valid_indices) < len(ground_truths):\n",
    "     print(f\"Warning: {len(ground_truths) - len(valid_indices)} examples had invalid ground truths and were excluded.\")\n",
    "     prompts = [prompts[i] for i in valid_indices]\n",
    "     ground_truths = [ground_truths[i] for i in valid_indices]\n",
    "     # If you need to keep track of original dataset items, adjust here\n",
    "     original_indices = valid_indices # Store the indices from the original dataset\n",
    "\n",
    "print(f\"Prepared {len(prompts)} prompts for evaluation.\")\n",
    "if len(prompts) > 0:\n",
    "    print(\"\\nExample Prompt:\")\n",
    "    print(prompts[0])\n",
    "    print(f\"Corresponding Ground Truth: {ground_truths[0]}\")\n",
    "else:\n",
    "    print(\"No valid prompts to evaluate.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:49:16.603884Z",
     "iopub.status.busy": "2025-04-11T15:49:16.603654Z",
     "iopub.status.idle": "2025-04-11T15:51:24.105003Z",
     "shell.execute_reply": "2025-04-11T15:51:24.104215Z",
     "shell.execute_reply.started": "2025-04-11T15:49:16.603862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:49:23 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:49:25.371252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744386565.554603      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744386565.605265      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading LLM with vLLM ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c05141446bb4271a5db22b6d5a1891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-11 15:49:39 [config.py:2704] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-11 15:49:52 [config.py:600] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 04-11 15:49:52 [arg_utils.py:1708] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 04-11 15:49:52 [arg_utils.py:1570] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
      "INFO 04-11 15:49:52 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-11 15:49:52 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged', speculative_config=None, tokenizer='MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f32baf920e41ef81495ccd174c3799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17062bade3b84d4099772b722a133b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2bcc9fc04e4c6984da40288fef70f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/495 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c46c7ff44f444bcbe994c5be144a287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:49:58 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-11 15:49:58 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W411 15:50:09.794407292 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:50:19 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-11 15:50:19 [model_runner.py:1110] Starting to load model MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W411 15:50:19.805167373 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:50:19 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf15a26509d6412397f4ed0b6cb33e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:50:33 [weight_utils.py:281] Time spent downloading weights for MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged: 13.589614 seconds\n",
      "INFO 04-11 15:50:33 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650c8bfb007249ceb6b7143bab9b3b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:50:36 [loader.py:447] Loading weights took 3.21 seconds\n",
      "INFO 04-11 15:50:37 [model_runner.py:1146] Model loading took 3.3461 GiB and 17.475646 seconds\n",
      "INFO 04-11 15:50:39 [worker.py:267] Memory profiling takes 1.34 seconds\n",
      "INFO 04-11 15:50:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 04-11 15:50:39 [worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.48GiB.\n",
      "INFO 04-11 15:50:39 [executor_base.py:112] # cuda blocks: 19855, # CPU blocks: 9362\n",
      "INFO 04-11 15:50:39 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 2.42x\n",
      "INFO 04-11 15:50:45 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:38<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 15:51:23 [model_runner.py:1598] Graph capturing finished in 38 secs, took 0.19 GiB\n",
      "INFO 04-11 15:51:23 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 46.68 seconds\n",
      "LLM 'MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged' loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# 3. Load Model using vLLM\n",
    "print(\"\\n--- Loading LLM with vLLM ---\")\n",
    "model_name = \"MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged\"\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "try:\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        # tensor_parallel_size=1,\n",
    "        # trust_remote_code=True,\n",
    "        # gpu_memory_utilization=0.9,\n",
    "        dtype=torch.float16,\n",
    "    )\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=4096,\n",
    "        temperature=0.01,\n",
    "        top_p=1.0,\n",
    "        top_k=-1\n",
    "    )\n",
    "    print(f\"LLM '{model_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM with vLLM: {e}\")\n",
    "    print(\"Please ensure the MODEL_ID is correct, vLLM is installed, and you have compatible hardware (GPU).\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:51:24.107046Z",
     "iopub.status.busy": "2025-04-11T15:51:24.106807Z",
     "iopub.status.idle": "2025-04-11T16:04:16.198100Z",
     "shell.execute_reply": "2025-04-11T16:04:16.196941Z",
     "shell.execute_reply.started": "2025-04-11T15:51:24.107029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Inference ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses: 100%|██████████| 25/25 [12:52<00:00, 30.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 responses.\n",
      "\n",
      "Example Generated Text (raw):\n",
      "Your reasoning should be concise and directly address the question.\n",
      "\"\n",
      "\n",
      "<think>\n",
      "Alright, let's figure out which tooth doesn't have 5 cusps. First, I know that the number of cusps on a tooth is pretty important for its function. More cusps mean the tooth is more robust and can handle more force. So, I'm thinking about the different types of teeth.\n",
      "\n",
      "Now, let's look at the options. We've got the mandibular 2nd, 1st, and 3rd molars, and the maxillary 1st molar. I remember that the mandibular molars, like the 2nd and 3rd, have 5 cusps. That's a pretty standard number for them.\n",
      "\n",
      "But what about the maxillary 1st molar? Hmm, I'm not as familiar with its cusps. I think it might have fewer cusps, maybe 4 or 5? I should double-check that.\n",
      "\n",
      "Oh, right! I've read somewhere that the maxillary 1st molar actually has 4 cusps. That's definitely different from the others. So, it seems like the maxillary 1st molar doesn't have 5 cusps.\n",
      "\n",
      "Let's just make sure I'm not missing anything. Yes, the mandibular molars definitely have 5 cusps. And the others, like the 2nd and 3rd molars, do too. So, it's pretty clear that the maxillary 1st molar is the odd one out here.\n",
      "\n",
      "So, after thinking it through, I'm confident that the maxillary 1st molar is the one without 5 cusps. That's the answer!\n",
      "</think>\n",
      "The maxillary 1st molar is the tooth that does not have 5 cusps. It typically has 4 cusps. Therefore, the correct answer is D. Maxillary 1st Molar.\n",
      "\n",
      "--- Extracting Predictions ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting choices: 100%|██████████| 100/100 [00:00<00:00, 6125.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not extract choice from text: '**Step 1:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 2:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 3:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 4:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 5:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 6:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 7:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 8:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 9:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 10:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 11:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 12:** What is the most common cause of a 26-year-old woman's symptoms?\n",
      "     **Step 13:** ...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning:\n",
      "     Your conclusion:\n",
      "     Answer: \n",
      "\n",
      "</think>\n",
      "To determine which medication can be used to delay preterm contractions of the uterus and improve the neurological outcome for the fetus, we need to evaluate the effects of each option.\n",
      "\n",
      "Mgsopine is a medication used to reduce the risk of preterm birth by decreasing the risk of preterm contractions. It works by inhibiting the action of the parasympathetic nervous system, which is responsible for the contractions. This can help to slow down the contractions and reduce the risk of preterm birth.\n",
      "\n",
      "Nifidipine is a medication used to reduce the risk of preterm birth by decreasing the risk of preterm contractions. It works by inhibiting the action of the sympathetic nervous system, which is responsible for the contractions. This can help to slow down the contractions and reduce the risk of preterm birth.\n",
      "\n",
      "Ritodrine is a medication used to reduce the risk of preterm birth by decreasing the risk of preterm contractions. It works by ...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning for the above question should be based on the definitions of the terms used in the question and the options.\n",
      "     Your reasoning should be clear and concise.\n",
      "     Your response should be in the form of a thought process, with each step explained.\n",
      "     Your final answer should be selected from the options given.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response should be in the form of a concise, well-structured sentence.\n",
      "     Your response shoul...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning should be concise and directly address the question.\n",
      "\"\n",
      "\n",
      "<think>\n",
      "Alright, let's think about this. We're looking for a condition that has both phosphaturia and osteomalacia. Phosphaturia is when there's too much phosphorus in the blood, and osteomalacia is when there's too much bone in the bones. So, we're looking for a condition that's causing both of these things.\n",
      "\n",
      "First, let's consider fibrosarcoma. It's a type of cancer that can happen in the bone marrow, and it's known for causing bone pain and swelling. But, I don't think it's known for causing both phosphaturia and osteomalacia. It's more about bone pain and swelling, not necessarily the high levels of phosphorus and bone.\n",
      "\n",
      "Next, let's think about osteosarcoma. This is a type of cancer that can occur in the bones, and it's known for causing osteomalacia, which is the bone swelling. But, does it cause phosphaturia? I don't think so. Osteosarcoma is more about bone swelling and bone pain, not necessarily high phosphor...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning for this question should be based on the characteristics of each stage of neurocysticercosis.\n",
      "\n",
      "To solve this, I need to understand the differences between the various stages of neurocysticercosis. I know that neurocysticercosis is a type of bacterial infection that can occur in the liver, and it's characterized by the presence of various types of edema. The stages of neurocysticercosis are divided into three categories: vesicular, vesicular colloidal, and granular nodular. Each stage has distinct characteristics, and the presence of edema varies accordingly.\n",
      "\n",
      "In the vesicular stage, the liver is characterized by the presence of a network of small, fluid-filled cells called vesicles. These vesicles are filled with a mixture of blood and serum, and they can cause edema. The presence of these vesicles is a key feature of the vesicular stage.\n",
      "\n",
      "In the vesicular colloidal stage, the liver is characterized by the presence of a network of small, fluid-filled cells called colloid...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning should be concise and directly address the question.\n",
      "\n",
      "The following is a table showing the properties of cardiolipin:\n",
      "Cardiolipin\n",
      "Molecular weight 120000 g/mole\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5%\n",
      "Chloride 1.5...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be:\n",
      "\n",
      "     Your reasoning here would be...'\n",
      "Warning: Could not extract choice from text: 'Your reasoning should be concise and directly address the question.\n",
      "     Your reasoning should be in the form of a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be a sentence or two.\n",
      "     Your response should be a concise answer to the question.\n",
      "     Your response should be...'\n",
      "Warning: Could not extract choice from text: '__\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     __\n",
      "     ...'\n",
      "\n",
      "### Number of invalid responces: 9\n",
      "\n",
      "Example Extracted Prediction:\n",
      "D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE=4\n",
    "\n",
    "# 4. Run Inference\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "all_outputs_text = []\n",
    "num_batches = math.ceil(len(prompts) / BATCH_SIZE)\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"Generating Responses\"):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(prompts))\n",
    "    batch_prompts = prompts[start_idx:end_idx]\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    # Note: vLLM's generate method handles batching internally based on available memory,\n",
    "    # but we iterate in logical batches here for progress tracking and potentially managing large datasets.\n",
    "    # However, for efficiency, you could potentially pass the entire `prompts` list directly to `llm.generate`\n",
    "    # if memory allows, letting vLLM handle the internal batching. We keep the loop for clarity.\n",
    "    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False) # Disable vllm's tqdm\n",
    "\n",
    "    # Extract the generated text for this batch\n",
    "    batch_outputs_text = [output.outputs[0].text.strip() for output in outputs]\n",
    "    all_outputs_text.extend(batch_outputs_text)\n",
    "\n",
    "print(f\"Generated {len(all_outputs_text)} responses.\")\n",
    "if len(all_outputs_text) > 0:\n",
    "    print(\"\\nExample Generated Text (raw):\")\n",
    "    print(all_outputs_text[0])\n",
    "\n",
    "# 5. Extract Predictions\n",
    "print(\"\\n--- Extracting Predictions ---\")\n",
    "predictions = [extract_choice(text) for text in tqdm(all_outputs_text, desc=\"Extracting choices\")]\n",
    "num_invalid_responces = predictions.count(None)\n",
    "print(f\"\\n### Number of invalid responces: {num_invalid_responces}\")\n",
    "\n",
    "if len(predictions) > 0:\n",
    "    print(\"\\nExample Extracted Prediction:\")\n",
    "    print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:04:16.199607Z",
     "iopub.status.busy": "2025-04-11T16:04:16.199092Z",
     "iopub.status.idle": "2025-04-11T16:04:16.224361Z",
     "shell.execute_reply": "2025-04-11T16:04:16.223807Z",
     "shell.execute_reply.started": "2025-04-11T16:04:16.199586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Metrics ---\n"
     ]
    }
   ],
   "source": [
    "# 6. Calculate Metrics\n",
    "print(\"\\n--- Calculating Metrics ---\")\n",
    "correct_count = 0\n",
    "total_count = len(predictions)\n",
    "results_by_subject = {} # For per-subject accuracy\n",
    "\n",
    "if total_count != len(ground_truths):\n",
    "     print(f\"Warning: Mismatch between number of predictions ({total_count}) and ground truths ({len(ground_truths)}). This should not happen.\")\n",
    "     # Adjust counts if necessary, though this indicates an earlier error\n",
    "     total_count = min(total_count, len(ground_truths))\n",
    "\n",
    "detailed_results = []\n",
    "\n",
    "for i in range(total_count):\n",
    "    # Use original_indices if subset was selected due to invalid ground truths\n",
    "    original_data_index = original_indices[i] if 'original_indices' in locals() else i\n",
    "    data_item = ds_medmcqa[original_data_index]\n",
    "    subject = data_item.get('subject_name', 'Unknown') # Handle missing subject\n",
    "\n",
    "    pred = predictions[i]\n",
    "    truth = ground_truths[i]\n",
    "\n",
    "    is_correct = (pred == truth)\n",
    "\n",
    "    if subject not in results_by_subject:\n",
    "        results_by_subject[subject] = {'correct': 0, 'total': 0}\n",
    "\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "        results_by_subject[subject]['correct'] += 1\n",
    "\n",
    "    results_by_subject[subject]['total'] += 1\n",
    "\n",
    "    # Store detailed results for potential analysis\n",
    "    detailed_results.append({\n",
    "        'id': data_item.get('id', f'index_{original_data_index}'),\n",
    "        'prompt': prompts[i],\n",
    "        'generated_text': all_outputs_text[i],\n",
    "        'prediction': pred,\n",
    "        'ground_truth': truth,\n",
    "        'subject': subject,\n",
    "        'is_correct': is_correct\n",
    "    })\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:04:16.225199Z",
     "iopub.status.busy": "2025-04-11T16:04:16.224989Z",
     "iopub.status.idle": "2025-04-11T16:04:16.255338Z",
     "shell.execute_reply": "2025-04-11T16:04:16.254791Z",
     "shell.execute_reply.started": "2025-04-11T16:04:16.225184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Model Evaluated: MilyaShams/DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged\n",
      "Dataset Used: medmcqa\n",
      "Number of Questions Evaluated: 100\n",
      "Number of Correct Answers: 18\n",
      "Overall Accuracy: 18.00%\n",
      "\n",
      "Accuracy by Subject:\n",
      "- Anaesthesia: 0.00% (0/1)\n",
      "- Anatomy: 0.00% (0/2)\n",
      "- Biochemistry: 0.00% (0/5)\n",
      "- Dental: 38.24% (13/34)\n",
      "- ENT: 50.00% (1/2)\n",
      "- Forensic Medicine: 0.00% (0/1)\n",
      "- Gynaecology & Obstetrics: 0.00% (0/12)\n",
      "- Medicine: 0.00% (0/3)\n",
      "- Microbiology: 0.00% (0/3)\n",
      "- Ophthalmology: 0.00% (0/2)\n",
      "- Pathology: 0.00% (0/3)\n",
      "- Pediatrics: 14.29% (1/7)\n",
      "- Pharmacology: 0.00% (0/7)\n",
      "- Physiology: 25.00% (1/4)\n",
      "- Radiology: 0.00% (0/2)\n",
      "- Social & Preventive Medicine: 50.00% (1/2)\n",
      "- Surgery: 10.00% (1/10)\n",
      "\n",
      "--- Saving Detailed Results (Optional) ---\n",
      "Detailed results saved to 'evaluation_results_DeepSeek-R1-Distill-Qwen-1.5B-medical-sft-merged_medmcqa.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_name=\"medmcqa\"\n",
    "\n",
    "# 7. Print Final Results\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Model Evaluated: {model_name}\")\n",
    "print(f\"Dataset Used: {dataset_name}\")\n",
    "print(f\"Number of Questions Evaluated: {total_count}\")\n",
    "print(f\"Number of Correct Answers: {correct_count}\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nAccuracy by Subject:\")\n",
    "# Sort subjects alphabetically for consistent output\n",
    "sorted_subjects = sorted(results_by_subject.keys())\n",
    "for subject in sorted_subjects:\n",
    "    counts = results_by_subject[subject]\n",
    "    sub_acc = (counts['correct'] / counts['total']) * 100 if counts['total'] > 0 else 0\n",
    "    print(f\"- {subject}: {sub_acc:.2f}% ({counts['correct']}/{counts['total']})\")\n",
    "\n",
    "# Optional: Save detailed results to a file\n",
    "print(\"\\n--- Saving Detailed Results (Optional) ---\")\n",
    "try:\n",
    "    results_df = pd.DataFrame(detailed_results)\n",
    "    output_filename = f\"evaluation_results_{model_name.split('/')[-1]}_{dataset_name.split('/')[-1] if dataset_name else 'local'}.csv\"\n",
    "    results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Detailed results saved to '{output_filename}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save detailed results to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
